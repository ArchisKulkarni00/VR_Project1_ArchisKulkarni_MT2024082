{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask segmentation using Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9383 9383 002267_2.jpg 002267_2.jpg\n"
     ]
    }
   ],
   "source": [
    "data_path = '../MSFD/1/face_crop/'\n",
    "ans_path = '../MSFD/1/face_crop_segmentation/'\n",
    "\n",
    "data_list = []\n",
    "ans_list = []\n",
    "\n",
    "for fname in os.listdir(data_path):\n",
    "    data_list.append(fname) \n",
    "\n",
    "\n",
    "for fname in os.listdir(ans_path):\n",
    "    ans_list.append(fname) \n",
    "\n",
    "print(len(data_list),len(ans_list),data_list[1],ans_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down = self.conv(x)\n",
    "        p = self.pool(down)\n",
    "\n",
    "        return down, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x1, x2], 1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.down_convolution_1 = DownSample(in_channels, 64)\n",
    "        self.down_convolution_2 = DownSample(64, 128)\n",
    "        self.down_convolution_3 = DownSample(128, 256)\n",
    "        self.down_convolution_4 = DownSample(256, 512)\n",
    "\n",
    "        self.bottle_neck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up_convolution_1 = UpSample(1024, 512)\n",
    "        self.up_convolution_2 = UpSample(512, 256)\n",
    "        self.up_convolution_3 = UpSample(256, 128)\n",
    "        self.up_convolution_4 = UpSample(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_1, p1 = self.down_convolution_1(x)\n",
    "        down_2, p2 = self.down_convolution_2(p1)\n",
    "        down_3, p3 = self.down_convolution_3(p2)\n",
    "        down_4, p4 = self.down_convolution_4(p3)\n",
    "\n",
    "        b = self.bottle_neck(p4)\n",
    "\n",
    "        up_1 = self.up_convolution_1(b, down_4)\n",
    "        up_2 = self.up_convolution_2(up_1, down_3)\n",
    "        up_3 = self.up_convolution_3(up_2, down_2)\n",
    "        up_4 = self.up_convolution_4(up_3, down_1)\n",
    "\n",
    "        out = self.out(up_4)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 72\n",
    "\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, image_folder, mask_folder):\n",
    "        super(MaskDataset, self).__init__()\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.image_filenames = os.listdir(image_folder)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_folder, self.image_filenames[idx])\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if image is None or mask is None:\n",
    "            print(f\"Warning: Failed to load {img_path} or {mask_path}\")\n",
    "            return None \n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        mask = Image.fromarray(mask)\n",
    "        mask = self.transform(mask)\n",
    "        mask = (mask > 0.5).float()  # Convert to binary mask\n",
    "\n",
    "        return image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 1876, Test Size: 7507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "full_dataset = MaskDataset(image_folder=data_path, mask_folder=ans_path)\n",
    "train_size = int(0.2 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Train Size: {train_size}, Test Size: {test_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (encoder1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder1): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (upconv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder2): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (final_conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)  # 128 -> 64\n",
    "\n",
    "        self.encoder2 = conv_block(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)  # 64 -> 32\n",
    "\n",
    "        self.bottleneck = conv_block(64, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # 32 -> 64\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)  # 64 -> 128\n",
    "        self.decoder2 = conv_block(64, 32)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        x = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(x)\n",
    "        x = self.pool2(enc2)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat([x, enc2], dim=1)\n",
    "        x = self.decoder1(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, enc1], dim=1)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        return torch.sigmoid(self.final_conv(x))\n",
    "\n",
    "\n",
    "# Create model instance\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "model = UNet(in_channels=3).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Unet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(prediction, target, epsilon=1e-07):\n",
    "    prediction_copy = prediction.clone()\n",
    "\n",
    "    prediction_copy[prediction_copy < 0] = 0\n",
    "    prediction_copy[prediction_copy > 0] = 1\n",
    "\n",
    "    intersection = abs(torch.sum(prediction_copy * target))\n",
    "    union = abs(torch.sum(prediction_copy) + torch.sum(target))\n",
    "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
    "    \n",
    "    return dice\n",
    "\n",
    "def iou_score(prediction, target, epsilon=1e-07):\n",
    "    prediction_copy = prediction.clone()\n",
    "\n",
    "    prediction_copy[prediction_copy < 0] = 0\n",
    "    prediction_copy[prediction_copy > 0] = 1\n",
    "\n",
    "    intersection = torch.sum(prediction_copy * target)\n",
    "    union = torch.sum(prediction_copy) + torch.sum(target) - intersection\n",
    "\n",
    "    iou = (intersection + epsilon) / (union + epsilon)\n",
    "\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/118 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 118/118 [01:25<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6986, IoU: 0.3162, Dice: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 118/118 [01:51<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.6482, IoU: 0.3165, Dice: 0.4805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 118/118 [01:38<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.6275, IoU: 0.3169, Dice: 0.4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 118/118 [01:43<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.6148, IoU: 0.3168, Dice: 0.4807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 118/118 [01:45<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.6074, IoU: 0.3167, Dice: 0.4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 118/118 [01:46<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.6017, IoU: 0.3162, Dice: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 118/118 [01:31<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.5990, IoU: 0.3170, Dice: 0.4810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 118/118 [01:36<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.5968, IoU: 0.3172, Dice: 0.4811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 118/118 [01:35<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.5955, IoU: 0.3164, Dice: 0.4803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 118/118 [01:31<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.5940, IoU: 0.3169, Dice: 0.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_iou = 0\n",
    "    epoch_dice = 0\n",
    "\n",
    "    # Training loop\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        if images is None or masks is None:\n",
    "            continue\n",
    "\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Compute IoU & Dice\n",
    "        iou = iou_score(outputs, masks)\n",
    "        dice = dice_coefficient(outputs, masks)\n",
    "        epoch_iou += iou.item()\n",
    "        epoch_dice += dice.item()\n",
    "\n",
    "    # Reduce learning rate based on scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print metrics for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}, IoU: {epoch_iou/len(train_loader):.4f}, Dice: {epoch_dice/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [01:32<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7280, IoU: 0.3310, Dice: 0.4969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [05:03<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.6931, IoU: 0.2438, Dice: 0.3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [04:41<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.6931, IoU: 0.1967, Dice: 0.3257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [04:25<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.6931, IoU: 0.1777, Dice: 0.2976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 128/147 [03:57<00:35,  1.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "losses = []\n",
    "dices = []\n",
    "ious = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_iou = 0\n",
    "    epoch_dice = 0\n",
    "\n",
    "    for images, masks in tqdm(train_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Compute IoU & Dice\n",
    "        iou = iou_score(outputs, masks)\n",
    "        dice = dice_coefficient(outputs, masks)\n",
    "        epoch_iou += iou.item()\n",
    "        epoch_dice += dice.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_iou = epoch_iou / len(train_loader)\n",
    "    avg_dice = epoch_dice / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, IoU: {avg_iou:.4f}, Dice: {avg_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"epoch\": num_epochs,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"loss\": loss\n",
    "}\n",
    "torch.save(checkpoint, \"unet_checkpoint.pth\")\n",
    "torch.save(model.state_dict(), \"unet_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder): Sequential(\n",
       "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (final_conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def predict_and_plot_from_loader(model, test_loader, device, idx = 0, plotIt = False):\n",
    "    # Get a random batch from test_loader\n",
    "    images, masks = next(iter(test_loader))\n",
    "        \n",
    "\n",
    "    # Get the selected image and mask\n",
    "    image = images[idx].to(device)  # Move to device\n",
    "    mask = masks[idx].cpu().numpy().squeeze()  # Convert mask to numpy\n",
    "\n",
    "    # Get model prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0))  # Add batch dimension\n",
    "        predicted_mask = output.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        predicted_mask = (predicted_mask > 0.5).astype(np.uint8)  # Threshold to binary\n",
    "\n",
    "    # Convert tensor image to NumPy (for visualization)\n",
    "    image_np = image.cpu().numpy().transpose(1, 2, 0)  # Change shape from (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Plot the image, ground truth mask, and model output\n",
    "\n",
    "    if(plotIt):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "        ax[0].imshow(image_np)\n",
    "        ax[0].set_title(\"Input Image\")\n",
    "        ax[0].axis(\"off\")\n",
    "\n",
    "        ax[1].imshow(mask, cmap=\"gray\")\n",
    "        ax[1].set_title(\"Ground Truth Mask\")\n",
    "        ax[1].axis(\"off\")\n",
    "\n",
    "        ax[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "        ax[2].set_title(\"Model Prediction\")\n",
    "        ax[2].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return mask, predicted_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(binary_image1, binary_image2):\n",
    "\n",
    "    # Ensure binary images are in boolean format (0 or 1)\n",
    "    mask1 = (binary_image1 > 0).astype(np.uint8)\n",
    "    mask2 = (binary_image2 > 0).astype(np.uint8)\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "\n",
    "    # Compute IoU (avoid division by zero)\n",
    "    iou = intersection / union if union != 0 else 0.0\n",
    "    return iou\n",
    "\n",
    "\n",
    "def compute_dice(binary_image1, binary_image2):\n",
    "\n",
    "    # Ensure binary images are in boolean format (0 or 1)\n",
    "    mask1 = (binary_image1 > 0).astype(np.uint8)\n",
    "    mask2 = (binary_image2 > 0).astype(np.uint8)\n",
    "\n",
    "    # Compute intersection and sum of pixels in both masks\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    total_pixels = mask1.sum() + mask2.sum()\n",
    "\n",
    "    # Compute Dice Score (avoid division by zero)\n",
    "    dice = (2 * intersection) / total_pixels if total_pixels != 0 else 0.0\n",
    "    return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.96862745].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOU:0.8483316481294236, DICE: 0.9179431072210066\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(images) - 1)\n",
    "mask, pred_mask = predict_and_plot_from_loader(model, test_loader, device, idx, True)\n",
    "iou, dice = compute_iou(mask, pred_mask), compute_dice(mask, pred_mask)\n",
    "\n",
    "print(f\"IOU:{iou}, DICE: {dice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average IOU:0.9144422491341103| Average DICE: 0.9547079594006832\n"
     ]
    }
   ],
   "source": [
    "net_iou, net_dice = 0, 0\n",
    "for idx in range(0, len(images) - 1):\n",
    "    mask, pred_mask = predict_and_plot_from_loader(model, test_loader, device, idx)\n",
    "    iou += compute_iou(mask, pred_mask)\n",
    "    dice += compute_dice(mask, pred_mask)\n",
    "\n",
    "print(f\"Average IOU:{iou/len(images)}| Average DICE: {dice/len(images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image, mask \u001b[38;5;241m=\u001b[39m \u001b[43mtest_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(image_tensor)\n",
    "    predicted_mask = output.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    predicted_mask = (predicted_mask > 0.5).astype(np.uint8)  # Threshold\n",
    "\n",
    "# Plot the image, ground truth mask, and model output\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title(\"Input Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(mask, cmap=\"gray\")\n",
    "ax[1].set_title(\"Ground Truth Mask\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "ax[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "ax[2].set_title(\"Model Prediction\")\n",
    "ax[2].axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load ../MSFD/1/face_crop/000601_1.jpg or ../MSFD/1/face_crop_segmentation/000601_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@51398.155] global loadsave.cpp:268 findDecoder imread_('../MSFD/1/face_crop_segmentation/000601_1.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m ious \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:139\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    137\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[1;32m    138\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melem_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:139\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[1;32m    138\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "test_iou = 0\n",
    "test_dice = 0\n",
    "\n",
    "losses = []\n",
    "dices = []\n",
    "ious = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        try:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compute IoU & Dice\n",
    "            iou = iou_score(outputs, masks)\n",
    "            dice = dice_coefficient(outputs, masks)\n",
    "            test_iou += iou.item()\n",
    "            test_dice += dice.item()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            dices.append(dice.item())\n",
    "            ious.append(iou.item())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "avg_test_iou = test_iou / len(test_loader)\n",
    "avg_test_dice = test_dice / len(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, IoU: {avg_test_iou:.4f}, Dice: {avg_test_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = list(range(1, num_epochs + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_list, losses, label='Training Loss')\n",
    "plt.xticks(ticks=list(range(1, num_epochs + 1, 1))) \n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_list, dices, label='Training DICE')\n",
    "plt.plot(epochs_list, ious, label='Validation DICE')\n",
    "plt.xticks(ticks=list(range(1, EPOCHS + 1, 1)))  \n",
    "plt.title('DICE Coefficient over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('DICE')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
